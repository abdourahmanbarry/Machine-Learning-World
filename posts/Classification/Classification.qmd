---
title: Classification
author: Abdourahman Barry
date: '2023-11-12'
categories:
  - code
  - analysis
freeze: false
jupyter: python3
---

### What is Classification?

Classification is the process of categorizing data into predefined classes. Classification is an example of supervised learning, where the model is trained on data with known classes. The goal of classification is to build and train a model with data to the point that it will be able to decern the class of future data without prior knowledge. Classification is similar to linear regression but the two differ in that linear regression outputs continuous values while classification model outputs are discrete and finite.

### Types of Classifiers

There are two different types of classifiers, namely Binary classifiers and multiclass classifiers. Examples of Binary classifiers include Stochastic Gradient Descent and SVM classifiers. On the other hand, K-Nearest Neighbors, Randomforest and Logistic Regression are examples of multiclass classifiers. In this blog post, we will explore the KNN classifier and Logistic Regression on a real dataset and valuate performance of this models using precision and recall curves.

### K-NearestNeighbor Classifier

The K-NearestNeighbor is a simple classification algorithm that predicts the class of a point base on the prevalent class of points closest to it. Given a new data point, the algorithm will find k points that are closes to the new data point and assign it to the class that is most represented among those k-points. In the following code, we will import a dataset, and visually use the KNN classifier to determine the classes of few new data points.

```{python}
#| fig-align: center
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pylab as plt

cls_data = pd.read_csv('classification1.csv')
plt.scatter(cls_data.x, cls_data.y, c=cls_data.color)
ax = plt.scatter([2, 4 , 4], [1.9, 1, 3], s=140, c='y')
plt.annotate('A', xy=(1.7, 1.5), size=20)
plt.annotate('B', xy=(4, 2.5), size=20)
_=plt.annotate('C', xy=(4, 0.5), size=20)
```

From the above plot, we want to classify the yellow points labeled A, B and C using K-NearestNeighbors. To do so, we will look at K number of points close to the point of interest and assign it to the dominant class. Since point B is much closer to the green class, the kNN algorithm will classify it as green. Similarly, point C will be classified as red. But point A might struggle a bit. Taking k to be 1, it will seem that A is much closer to red. However, increasing k, it might be classified in the blue category.

One of the problems associated with K-Nearest Neighbors is the fact that there is a possibility to have a tie if k is not chosen wisely. The way to deal with this is either to make a choice about what to do in such cases or use an odd number for k in the case of binary classification.

Now lets look at how effective the k-Nearest Neighbors is on a real dataset. Here, we will use the diabetes dataset to develop a model to predict whether a patient has diabetes or not. First of all, we import and perform some exploration on the data.

```{python}
#| fig-align: center
#| layout: "[800]"
diabetes = pd.read_csv('diabetes.csv')
data, targets = diabetes[list(diabetes.columns)[0:-1]], diabetes['Outcome']
data
_=sns.pairplot(diabetes, hue='Outcome')
```

From this plots, we can see that diabetes patients seem to have high glucose, BMI, Insulin and Age, whiles it is the opposite trend for non-diabetes patients. We can look at this features much closer.

```{python}
#| fig-align: center
#| layout: "[500]"
fig, axes = plt.subplots(1, 3)
fig.set_size_inches(13, 5, forward=True)
sns.scatterplot(x='Glucose', y='BMI',data=diabetes, hue='Outcome', ax=axes[0])
sns.scatterplot(x='Glucose', y='Insulin',data=diabetes, hue='Outcome', ax=axes[1])
_= sns.scatterplot(x='Glucose', y='Age',data=diabetes, hue='Outcome', ax=axes[2])
```

Motivated by such trends, we will train a KNN model on only this features and see how well it will perform.

```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import seaborn as sns

training_x, test_x, training_y, test_y = train_test_split(data[['BMI', 'Age', 'Insulin', 'Glucose']], targets)
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(training_x, training_y)
pred = knn.predict(test_x)

print(classification_report(test_y, pred))
```

This model did not do very on this data. Both recall and precision are low and the accuracy of the model is not high enough. Next, we will use all the features of the dataset to train the model.

```{python}
training_x, test_x, training_y, test_y = train_test_split(data, targets)
knn = KNeighborsClassifier(n_neighbors=8)
knn.fit(training_x, training_y)
pred = knn.predict(test_x)

print(classification_report(test_y, pred))
```

We get a slightly better performs when we used all the features in our data. Lets look at a precision/recall trade-off that we can make for this model. Let us imagine that we are building this model to be used in a triage section to filter patients who would need to go on a proper diagonostic check-up for diabetes. In this case recall score will be the most important. It is okay to diagnose some people with diabetes wrongly, since there is a chance that will be overturned by the second round of testing, which we assume will have higher accuracy. So we want to have as few false negatives as possible. Therefore, we need a very good recall score. Usually, we get the right recall score by finding the appropriate value for k.

```{python}
#| fig-align: center
#We will choice k from 1 to 30 in the KNN algorithms and calculate both precisio and recall
from sklearn.metrics import precision_score, recall_score
precision = []
recall = []
m = 31
for k in range(1, m):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(training_x, training_y)
    pred = knn.predict(test_x)
    rec = recall_score(test_y, pred)
    prec = precision_score(test_y, pred)
    recall.append(rec)
    precision.append(prec)
    #print(precision_score(test_y, pred))
plt.plot(range(1, m), recall, 'o--' , range(1, m), precision, 'o--')
plt.legend(['recall', 'precision'])
_ = plt.xlabel('K neigbors')
    
```

As we can see, the k nearest neighbor has a poor performance as far as the recall score is concerned for various values of k. this will not be suitable for the kind of application that we want. So we need another model.

### Logistic Regression

The K-Nearest Neighbor performance was not adquate for our purposes. Can logistic Regression do better? In the following code, we will use logistic regression algorithm with different thresholds and pick the one with high recall score.

```{python}
#| fig-align: center
#| layout: "[500]"
from sklearn.linear_model import LogisticRegression
import warnings
warnings.filterwarnings("ignore")

log = LogisticRegression(random_state=42)
log.fit(training_x, training_y)
pred = log.predict_proba(test_x)

binarize =  lambda x, t: np.array([1 if i>=t else 0 for i in x ]) #this is to turn the outputs into 0 or 1
test_y = np.array(test_y)

precision = []
recall = []
thresholds = np.linspace(0, 1, 100)
for i in thresholds:
    log = LogisticRegression(random_state=42)
    log.fit(training_x, training_y)
    pred = log.predict_proba(test_x)
    val = binarize(pred[:,1],i)
    rec = recall_score(test_y, val)
    prec = precision_score(test_y, val)
    precision.append(prec)
    recall.append(rec)

fig, axes = plt.subplots(1, 3)
fig.set_size_inches(13, 5, forward=True)
plt.subplot(1, 3, 1)
plt.plot(thresholds, recall)
plt.title('Recall Curve')
plt.xlabel('Threshold')
plt.ylabel('Recall')
plt.subplot(1, 3, 2)
plt.plot(thresholds, precision)
plt.title('Precision Curve')
plt.xlabel('Threshold')
plt.ylabel('Precision')
plt.subplot(1, 3, 3)
plt.plot(recall, precision, label='Precision/Recall curve')
plt.title('Precision/Recall(PR) Curve')
_ = plt.xlabel('Recall')
```

It seems like choicing the threshold for the logistic regression model to be 2 will give a high recall score for the model and the sacrifices in precision is not bad either. This will ensure that we have as few false negatives as possible, which is what is more important in this situation.

# Conclusion

In this blog post, we have seen two classification models: K-Nearest Neigbors and Logistic Regression. We have compared both models and found out that logistic regression is more suitable for our application requirements.
