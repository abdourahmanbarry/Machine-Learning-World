[
  {
    "objectID": "posts/Random Variables/Random Variables.html",
    "href": "posts/Random Variables/Random Variables.html",
    "title": "Random Variables",
    "section": "",
    "text": "What is a Random Variable?\nA random variable is simply a mapping from a sample space of a random experiment to the real line. Imagine performing an experiment, say throwing a die twice, you might be interested in a numerical value such as the following :\n\nNumber of six oberved\nThe sum of the outcomes of the two rolls\neven numbers observed\n\nThis are all examples of random variables of our experiment. One thing that we can observe is that from one experiment we can have different random variable and each one is also not surprisely random because its values are derived from the sample space which is itself random.\nLets demonstrate this idea by performing a simple experiment of throwing a die 100 times and counting the number of heads observed. If we repeat this experiment a large number of times, how many number of heads would we likely to observe? Notice that here the number of heads observed for each experiment(tose of a die 100 times) is a random variable that can take values from 0 t0 100. The following code does exactly this.\n\nimport numpy as np\nfrom matplotlib import pylab as plt\nimport pandas as pd\n\nheads = []\n\nfor i in range(5000):\n    outcome  = np.sum(np.random.randint(0, 2, 100))\n    heads.append(outcome)\n\n_ = plt.hist(heads, 100)\n\n\n\n\n\n\n\n\nWe can see that repeating this experiment a number of times, we are more likely to observe 50 heads and values closed to it for each run of the experient, while values closed to 0 or 100 would be rarely observed. In fact, this is an example of Binomial Random variable. As you can see this distribution has the “famous” bell shape, which is ubiquitous in number applications. We will study the continuous analog called the Gaussian distribution.\n\n\nGaussian Random Variable\nA random variable is called Gaussian if it has the following probability density function\n\\[\\large{f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp\\displaystyle \\{ {-\\frac{(x - \\mu)^2}{2\\sigma^2}}} \\}\\]\nwhere \\(\\sigma\\) is the standard deviation and \\(\\mu\\) is the mean of the distribution. The Gaussian distribution has the following shape.\n\ndef pdf(x, u, s):\n    p1 = 1/(np.sqrt(2*np.pi)*s)\n    p2 = - ((x - u)**2)/(2*s**2)\n    return p1*np.e**p2\n\nx = np.arange(-4, 4, 0.25)\ny = pdf(x, 0, 1)\n_ = plt.plot(x, y)\n\n\n\n\n\n\n\n\nThe width of this curve is controlled by the value of \\(\\sigma\\) and the center point is controlled by \\(\\mu\\). We can see this in the next plots.\n\nparameters = [(0, 1), (7, 1 ), (15, 0.5), (15, 1.4)]\nf = plt.figure(figsize=(15,7))\nfor i in range(4):\n    u, s = parameters[i]\n    x = np.arange(u-4, u+4, 0.1)\n    y = pdf(x, u , s)\n    plt.subplot(2, 2, i+1)\n    plt.ylim([0, 0.9])\n    plt.plot(x, y)\n    _ = plt.title(f's = {s}, u={u}')\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected the width of the curve increases with \\(\\sigma\\) and the center of the curve moves to wherever the mean is. However, the height of the curve is always inversely proportional to \\(\\sigma\\). As the value of the standard deviation increases, the height of the curve decreases and vice-versa.\n\n\nConclusion\nIn conclusion, the gaussian distribution is very common and it is used a lot in practice. In our last blog post we will look at outlier detection where the idea of Gaussian distribution will be used to create a machine learning algorithm for outlier detection, which has a number of uses in practice."
  },
  {
    "objectID": "posts/Linear Regression/Linear Regression.html",
    "href": "posts/Linear Regression/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "What is Linear Regression?\nLinear Regression is a process of estimating a line that best fits the general trend of some data. The goal of linear regression is given a data, we want to find the best curve that give a very good estimate of that data. Therefore, it is very important to visualize the data to see what are the trends before attempting to fit the data. Fitting a data with a line when there is no linear relationship will result in a bad model.\n\n\n\n\n\nThe data in plot A seem to have linear correlation between the x and y variable, so it makes sense to fit it with a line. However, plot B does not seem to have any correlation between the two variables, so it would be a bad idea to fit it with a line.\n\n\nHow to find the line of best-fit?\nWe will use the LinearRegression model in Sklearn to find the line of best fit. We will import our data and pass it to our algorithm to find the best parameters for our model.\n\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pylab as plt\n\ndata = pd.read_csv('Linear_regression_data1.csv')\n\nx = np.array(data.x).reshape(-1, 1)\ny = np.array(data.y).reshape(-1, 1)\n\nplt.scatter(x , y)\n\nlreg = LinearRegression()\nlreg.fit(x, y)\n\nnewx = np.linspace(0, 17, 100).reshape(-1, 1)\nnewy = lreg.predict(newx)\n\n_=plt.plot(newx, newy, c='r', linewidth=3)\n\n\n\n\n\n\n\n\nAs you can see, this line seem to perfectly fit this data. For every value of the dependent variable x, we can now use the model to estimate the corresponding value of the independent variable y, which is the typical use case for linear models.\n\n\nNon-Linear Regression\nAs you can imagine, not all variables have a linear relationship. The data on the plot below shows one such example.\n\nimport pandas as pd\nfrom matplotlib import pylab as plt\n\ndata = pd.read_csv('Linear_regression_data3.csv')\n_ = plt.scatter(data.x, data.y)\n\n\n\n\n\n\n\n\nAttempting to fit this data with a line will simply underfit the data. This means that our model will perform poorly on the training data, and would not likely generalize to instances it has never seen before. As you can see, our model will only predict few instance correctly.\n\nimport numpy as np\nfrom matplotlib import pylab as plt\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv('Linear_regression_data3.csv')\n\nreg = LinearRegression()\nx = np.array(data.x).reshape(-1, 1)\ny = np.array(data.y).reshape(-1, 1)\nplt.scatter(x, y)\nreg.fit(x, y)\n\n\n\nxnew = np.linspace(-10, 10, 100).reshape(-1, 1)\nynew = reg.predict(xnew)\n_ = plt.plot(xnew, ynew, linewidth=3, c='r')\n\n\n\n\nFigure 2.1; Example of model underfitting\n\n\n\n\nWe need to fit the data with something better than just a simple line. If you closely inspect the data, we can see that it has a shape that closely resembles a parabola. So we can use sklearn polynomialFeatures to actually fit the data with a polynomial of degree 2. The following code fit the data with a polynomial instead.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nreg = make_pipeline(PolynomialFeatures(2), LinearRegression())\nreg.fit(x, y)\nplt.scatter(x, y)\n\n\nxnew = np.linspace(-10, 10, 100).reshape(-1, 1)\nynew = reg.predict(xnew)\n_=plt.plot(xnew, ynew, c='r', linewidth=2)\n\n\n\n\n\n\n\n\nThe resulting curve seem to fit the data very well and it will generalize much better than the linear model we tried earlier. In fact, this data was generated using the quadratic equation \\(y = x^2 - 4x + 3\\) with gaussian noise.\n\n\nUnderfitting and Overfitting of Data\nOne example of under-fitting is figure 2.1. In this instance we can see that the line is a poor estimate for the data. It deviated badly for large portions of the dataset. It is safe to assume that it will also not generalize very well on the test data. Indeed, this is the characteristic of a model that over-fits. It will perform poorly on both the training and test set.\nOn the other hand, if we were to use a polynomial of a very high degree to fit this data, it will be highly accurate on the training data, but it will not generalize too well on the training set. So when a model performs very well on the training set and poorly on the test set, we say that the model is overfiting the data."
  },
  {
    "objectID": "posts/Classification/Classification.html",
    "href": "posts/Classification/Classification.html",
    "title": "Classification",
    "section": "",
    "text": "What is Classification?\nClassification is the process of categorizing data into predefined classes. Classification is an example of supervised learning, where the model is trained on data with known classes. The goal of classification is to build and train a model with data to the point that it will be able to decern the class of future data without prior knowledge. Classification is similar to linear regression but the two differ in that linear regression outputs continuous values while classification model outputs are discrete and finite.\n\n\nTypes of Classifiers\nThere are two different types of classifiers, namely Binary classifiers and multiclass classifiers. Examples of Binary classifiers include Stochastic Gradient Descent and SVM classifiers. On the other hand, K-Nearest Neighbors, Randomforest and Logistic Regression are examples of multiclass classifiers. In this blog post, we will explore the KNN classifier and Logistic Regression on a real dataset and valuate performance of this models using precision and recall curves.\n\n\nK-NearestNeighbor Classifier\nThe K-NearestNeighbor is a simple classification algorithm that predicts the class of a point base on the prevalent class of points closest to it. Given a new data point, the algorithm will find k points that are closes to the new data point and assign it to the class that is most represented among those k-points. In the following code, we will import a dataset, and visually use the KNN classifier to determine the classes of few new data points.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pylab as plt\n\ncls_data = pd.read_csv('classification1.csv')\nplt.scatter(cls_data.x, cls_data.y, c=cls_data.color)\nax = plt.scatter([2, 4 , 4], [1.9, 1, 3], s=140, c='y')\nplt.annotate('A', xy=(1.7, 1.5), size=20)\nplt.annotate('B', xy=(4, 2.5), size=20)\n_=plt.annotate('C', xy=(4, 0.5), size=20)\n\n\n\n\n\n\n\n\nFrom the above plot, we want to classify the yellow points labeled A, B and C using K-NearestNeighbors. To do so, we will look at K number of points close to the point of interest and assign it to the dominant class. Since point B is much closer to the green class, the kNN algorithm will classify it as green. Similarly, point C will be classified as red. But point A might struggle a bit. Taking k to be 1, it will seem that A is much closer to red. However, increasing k, it might be classified in the blue category.\nOne of the problems associated with K-Nearest Neighbors is the fact that there is a possibility to have a tie if k is not chosen wisely. The way to deal with this is either to make a choice about what to do in such cases or use an odd number for k in the case of binary classification.\nNow lets look at how effective the k-Nearest Neighbors is on a real dataset. Here, we will use the diabetes dataset to develop a model to predict whether a patient has diabetes or not. First of all, we import and perform some exploration on the data.\n\ndiabetes = pd.read_csv('diabetes.csv')\ndata, targets = diabetes[list(diabetes.columns)[0:-1]], diabetes['Outcome']\ndata\n_=sns.pairplot(diabetes, hue='Outcome')\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this plots, we can see that diabetes patients seem to have high glucose, BMI, Insulin and Age, whiles it is the opposite trend for non-diabetes patients. We can look at this features much closer.\n\nfig, axes = plt.subplots(1, 3)\nfig.set_size_inches(13, 5, forward=True)\nsns.scatterplot(x='Glucose', y='BMI',data=diabetes, hue='Outcome', ax=axes[0])\nsns.scatterplot(x='Glucose', y='Insulin',data=diabetes, hue='Outcome', ax=axes[1])\n_= sns.scatterplot(x='Glucose', y='Age',data=diabetes, hue='Outcome', ax=axes[2])\n\n\n\n\n\n\n\n\n\n\n\n\nMotivated by such trends, we will train a KNN model on only this features and see how well it will perform.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nimport seaborn as sns\n\ntraining_x, test_x, training_y, test_y = train_test_split(data[['BMI', 'Age', 'Insulin', 'Glucose']], targets)\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(training_x, training_y)\npred = knn.predict(test_x)\n\nprint(classification_report(test_y, pred))\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.77      0.80       132\n           1       0.56      0.65      0.60        60\n\n    accuracy                           0.73       192\n   macro avg       0.69      0.71      0.70       192\nweighted avg       0.74      0.73      0.73       192\n\n\n\nThis model did not do very on this data. Both recall and precision are low and the accuracy of the model is not high enough. Next, we will use all the features of the dataset to train the model.\n\ntraining_x, test_x, training_y, test_y = train_test_split(data, targets)\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(training_x, training_y)\npred = knn.predict(test_x)\n\nprint(classification_report(test_y, pred))\n\n              precision    recall  f1-score   support\n\n           0       0.81      0.87      0.84       134\n           1       0.64      0.52      0.57        58\n\n    accuracy                           0.77       192\n   macro avg       0.72      0.70      0.71       192\nweighted avg       0.76      0.77      0.76       192\n\n\n\nWe get a slightly better performs when we used all the features in our data. Lets look at a precision/recall trade-off that we can make for this model. Let us imagine that we are building this model to be used in a triage section to filter patients who would need to go on a proper diagonostic check-up for diabetes. In this case recall score will be the most important. It is okay to diagnose some people with diabetes wrongly, since there is a chance that will be overturned by the second round of testing, which we assume will have higher accuracy. So we want to have as few false negatives as possible. Therefore, we need a very good recall score. Usually, we get the right recall score by finding the appropriate value for k.\n\n#We will choice k from 1 to 30 in the KNN algorithms and calculate both precisio and recall\nfrom sklearn.metrics import precision_score, recall_score\nprecision = []\nrecall = []\nm = 31\nfor k in range(1, m):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(training_x, training_y)\n    pred = knn.predict(test_x)\n    rec = recall_score(test_y, pred)\n    prec = precision_score(test_y, pred)\n    recall.append(rec)\n    precision.append(prec)\n    #print(precision_score(test_y, pred))\nplt.plot(range(1, m), recall, 'o--' , range(1, m), precision, 'o--')\nplt.legend(['recall', 'precision'])\nplt.xlabel('K neigbors')\n\nText(0.5, 0, 'K neigbors')\n\n\n\n\n\n\n\n\n\nAs we can see, the k nearest neighbor has a poor performance as far as the recall score is concerned for various values of k. this will not be suitable for the kind of application that we want. So we need another model.\n\n\nLogistic Regression\nThe K-Nearest Neighbor performance was not adquate for our purposes. Can logistic Regression do better? In the following code, we will use logistic regression algorithm with different thresholds and pick the one with high recall score.\n\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nlog = LogisticRegression(random_state=42)\nlog.fit(training_x, training_y)\npred = log.predict_proba(test_x)\n\nbinarize =  lambda x, t: np.array([1 if i&gt;=t else 0 for i in x ]) #this is to turn the outputs into 0 or 1\ntest_y = np.array(test_y)\n\nprecision = []\nrecall = []\nthresholds = np.linspace(0, 1, 100)\nfor i in thresholds:\n    log = LogisticRegression(random_state=42)\n    log.fit(training_x, training_y)\n    pred = log.predict_proba(test_x)\n    val = binarize(pred[:,1],i)\n    rec = recall_score(test_y, val)\n    prec = precision_score(test_y, val)\n    precision.append(prec)\n    recall.append(rec)\n\nfig, axes = plt.subplots(1, 3)\nfig.set_size_inches(13, 5, forward=True)\nplt.subplot(1, 3, 1)\nplt.plot(thresholds, recall)\nplt.title('Recall Curve')\nplt.xlabel('Threshold')\nplt.ylabel('Recall')\nplt.subplot(1, 3, 2)\nplt.plot(thresholds, precision)\nplt.title('Precision Curve')\nplt.xlabel('Threshold')\nplt.ylabel('Precision')\nplt.subplot(1, 3, 3)\nplt.plot(recall, precision, label='Precision/Recall curve')\nplt.title('Precision/Recall(PR) Curve')\nplt.xlabel('Recall')\n\n\n\nText(0.5, 0, 'Recall')\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems like choicing the threshold for the logistic regression model to be 2 will give a high recall score for the model and the sacrifices in precision is not bad either. This will ensure that we have as few false negatives as possible, which is what is more important in this situation.\n\n\nConclusion\nIn this blog post, we have seen two classification models: K-Nearest Neigbors and Logistic Regression. We have compared both models and found out that logistic regression is more suitable for our application requirements."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nMy name is Abdourahman Barry, a graduate student in the CS department at Virginia Tech working towards my PhD. This blog post is part of the requirements for completing the intro to Machine Learning course and this is my take on what I have been able to learn this semester.\nPlease, find the Github repository for this project below."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning World",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nAbdourahman Barry\n\n\n\n\n\n\n  \n\n\n\n\nOutlier Detection\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nAbdourahman Barry\n\n\n\n\n\n\n  \n\n\n\n\nRandom Variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nAbdourahman Barry\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nAbdourahman Barry\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nAbdourahman Barry\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Clustering/Clustering.html",
    "href": "posts/Clustering/Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in machine learning is a way of grouping a data into catagories where each category contains points with similar properties. This is an example of unsupervised learning because the dataset is not label. Clustering algorithm have a number of uses such as fraud detection, medical discovery and so on. We will demonstrate one of the applications of clustering algorithms in a later post for anomaly detection.\n\n\n\n\n\nBy observing the plot, we can see that our data has three groups. This groups are called clusters. So the points in cluster A have more similarities than those in B or C. The job of a clustering algorithm is to find such clusters in our data. Examples of clustering algorithm includes K-means, DBSCAN and Gaussian-Mixtures. In this blog post we will explore only the K-means algorithm."
  },
  {
    "objectID": "posts/Clustering/Clustering.html#kmeans-algorithm",
    "href": "posts/Clustering/Clustering.html#kmeans-algorithm",
    "title": "Clustering",
    "section": "KMeans Algorithm",
    "text": "KMeans Algorithm\nHow does KMeans work? Well, for a start, KMeans will setup k-random number of points which it calls centroids. In each iteration, the algorithm will assign each point in the dataset to the nearest centroid and updates the centroids to be the mean of the points assigned to it. The algorithm then repeats the process until the centroids stop changing. Those final centroids will then be the center of the clusters. The following code demonstrates how to use the KMeans algorithm in sklearn.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import scale, StandardScaler\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\ndata = pd.read_csv('knn_demonstration_data.csv')\nsc = StandardScaler()\nx_trans = sc.fit_transform(data[['x_values', 'y_values']])\nx_y_frame = pd.DataFrame(x_trans)\nx_y_frame.columns = ['x', 'y']\nkm = KMeans(n_clusters=3)\nkm.fit(x_y_frame)\n\n\nx_y_frame['labels'] = km.labels_\nc = km.cluster_centers_\nsns.scatterplot(x='x', y='y', data=x_y_frame, hue='labels')\n_ = sns.scatterplot(x=c[:, 0], y=c[:, 1],  marker='x', linewidth=5, s=200)\n\n\n\n\nAs you can see, the KMeans algorithms successfully partitioned our data into three categories. Each of the clusters identified by a centroid, which can be visibly seen in the center of each cluster.\n\nHow to determine K for KMeans?\nOne issue with KMeans is that it requires that it be told how many clusters to find. For large datasets, visualization may turnout to be hard and hence the problem of telling what is a suitable value of K. However, we can try a number of values for K and pick the smallest one that minimizes the mean squared error (MSE). To do this, we create what is called an “elbow-plot”, using some dataset.\n\nfrom matplotlib import pylab as plt\nimport warnings\nwarnings.simplefilter('ignore')\n\ndata = pd.read_csv('clustering_elbow_data.csv')\nsc = StandardScaler()\nx_trans = sc.fit_transform(data)\nx_y_frame = pd.DataFrame(x_trans)\nx_y_frame.columns = ['x', 'y']\n\nmse = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i)\n    km.fit(x_y_frame)\n    mse.append(km.inertia_)\n\nplt.plot(range(1, 11), mse, 'o--')\nplt.title('Mean Squared Error vs K')\nplt.ylabel('MSE')\nplt.xlabel('K')\nplt.grid('on')\n\n\n\n\nIt can be observed that at k=4 the curve starts to flatted. This suggests that K=4 might just be the right value for this problem. The following visualization will make this clear.\n\n_= sns.scatterplot(x='x', y='y', data=x_y_frame)\n\n\n\n\nIndeed, this plot confirms that we have four clearly separated clusters in this data."
  },
  {
    "objectID": "posts/Outlier Detection/Outlier Detection.html",
    "href": "posts/Outlier Detection/Outlier Detection.html",
    "title": "Outlier Detection",
    "section": "",
    "text": "What is Outlier Dectection?\nOutlier detection is a way of figuring out data points that do not follow an established pattern. Once we observe a pattern in our data that represence normal bahaviour, then it is easy to flag the points that do not conform to this pattern as being anomalies or generated by different underleing mechanism. This turns out to be very important in number of fields, such as in medicine, drug discovery and scurity such as faud detection. In this blog post we will examine how to build a simple outlier detection algorithm.\n\n\n\n\n\nimg&gt;In the plot above, we can see that most of the data are clustered together we and few are scattered far away from the cluster center. From an outlier detection perspective, we can assume that the points in the center, which represents majority of the data, are normal whiles those far away noisy data, and hence we can label them as anomalies. This is the bases of outlier detectors.\nLets recall what we learn from the first blog post. We show that the gaussian distribution has a bell-shaped curve like the one below.\n\n\n\nfig 2: Gaussian Distribution\n\n\nFrom this curve, we can see that points that are far from the mean have low probabilties(are less common) while points close to the center have high probabilities(more common). The function that estimates this probabilities called is probability density function is given as: \\[\\large{P(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} exp\\displaystyle\\{ {-\\frac{(x - \\mu)^2}{2\\sigma^2}}} \\}\\]\nTherefore if we know that our data has Gaussian noise we can simply estimate a probility density function \\(P(x)\\) of the data. Using an appropriate threshold, say \\(\\mu\\), we can determine anomalies/normal data points if they are below/above the threshold.\n\\[f(x)  = \\left\\{\\begin{array}{cr}\n            0 & \\text{if } P(x) \\leq  \\mu\\\\[0.3cm]\n            1 & \\text{if } P(x) &gt; \\mu\\\\\n        \\end{array} \\right.\\]\nWe can extend this idea if we have n number of features by assumming independence and writting out the joint probability density function as follows: \\[\\large{P(x_1, x_2, \\cdots , x_n) =  \\prod_{i = 1}^{n}  \\frac{1}{\\sqrt{2\\pi}\\sigma_i} exp \\displaystyle\\{ {-\\frac{(x_i - \\mu_i)^2}{2\\sigma_i^2}}} \\}\\]\nWe can apply this algorithm that we just found to the data in the first plot to do outlier detection. Let’s make joint plot to show that the features in this data are gaussian.\n\nimport numpy as np\nfrom matplotlib import pylab as plt\nimport seaborn as sns\nimport pandas as pd\n\n\ndata = pd.read_csv('Outlier_example4.csv')\n_=sns.jointplot(x='x', y='y', data=data)\n\n\n\n\n\n\n\n\nOn this plot, we can see the marginal density plots for each of our the features, and both of them are Gaussian. We can then proceed to implement our simple outlier detector.\n\ndef pdf(x, u, s):\n    p1 = 1/(np.sqrt(2*np.pi)*s)\n    p2 = - ((x - u)**2)/(2*s**2)\n    return p1*np.e**p2\n\n\nthreshold = 0.00035\ndata['class'] = None\nx_mean = data.x.mean()\nx_std = data.x.std()\ny_mean = data.y.mean()\ny_std = data.y.std()\n\nfor index, row in data.iterrows():\n    x = row['x']\n    y = row['y']\n    value = pdf(x, x_mean, x_std)*pdf(y, y_mean, y_std)\n    #print(value)\n    outcome = ''\n    if value &lt; threshold:\n        outcome = 'Outlier'\n    else:\n        outcome = 'Normal'   \n    data.at[index, 'class'] = outcome\n\ng=sns.scatterplot(x='x', y='y',data=data, hue='class', size=500)\n\n\n\n\n\n\n\n\nYou can see the outcome of running our simple detection algorithm. Most of the farthest points have been flag (orange) for being anomalies and that is what we expected.\n\n\nConclusion\nIn this blog post, we have learned how to build a simple outlier detection algorithm and applied it to real data. Sklearns implementation is more advance than this our algorithm but the underlying idea remains the same."
  }
]